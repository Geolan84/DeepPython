Здравствуйте.

Так как файл со списком ссылок может быть чрезмерно огромным, я реализовал генератор, читающий файл построчно.
После чего возникла проблема: если создавать "task" на каждую ссылку, то по сути весь файл всё равно окажется в памяти,
а этого я изначально и пытался избежать. Тогда я решил сделать одну task для обновления очереди ссылок
и n "tasks" для обращения к ссылкам, однако при таком подходе из-за недетерменизма в некоторых случаях ссылок может стать больше,
чем нужно, поэтому решение стало следующим:

    def process_links()
    1) создаёт фиксированное число воркеров
    2) инициирует цикл
    3) на каждой итерации заполняет очередь на фиксированное количество ссылок без ожидания завершения вставки (так быстрее работает)
    4) ждёт, когда воркеры асинхронно прозвонят все ссылки из очереди, и переходит на следующую итерацию

То есть решение асинхронное, но при этом итерационное (то есть ссылки порционно обрабатываются в асинхронном режиме)